{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartkerb02/Doc-Query-Agent/blob/main/NLQueryAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pfcXaiDY0p5D"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q replicate\n",
        "!pip install -q chromadb\n",
        "!pip install -q pypdf\n",
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "0LrU5ytT1UMw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "yz18bAOGX-o6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_6bsJkXCY7GX2IBvTniWBEXsJIN1wyFC0uegtG\""
      ],
      "metadata": {
        "id": "nHME6TXcw2Yj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "\n",
        "pdf_folder_path = f'./data/'\n",
        "\n",
        "document=[]\n",
        "for file in os.listdir(pdf_folder_path):\n",
        "  file_path = pdf_folder_path+file\n",
        "  if file.endswith(\".pdf\"):\n",
        "    loader=PyPDFLoader(file_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "    loader=Docx2txtLoader(file_path)\n",
        "    document.extend(loader.load())\n",
        "  elif file.endswith('.txt'):\n",
        "    loader=TextLoader(file_path)\n",
        "    document.extend(loader.load())\n"
      ],
      "metadata": {
        "id": "-ual8vMm37hB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_splitter=CharacterTextSplitter(separator='\\n', chunk_size=500, chunk_overlap=80)\n",
        "document_chunks=document_splitter.split_documents(document)\n",
        "len(document_chunks)"
      ],
      "metadata": {
        "id": "RScttgIJVIej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f1d222-4abd-47b2-a644-4a6d8ef5ed9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.utils import embedding_functions\n",
        "default_ef = embedding_functions.DefaultEmbeddingFunction()"
      ],
      "metadata": {
        "id": "6zpHSD4RSxMv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "IJNClKAsWH0S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db2 = Chroma.from_documents(document_chunks, embeddings, persist_directory=\"./chroma_db\")\n",
        "db2.persist()\n",
        "\n",
        "my_retriever = db2.as_retriever(search_kwargs={'k':2})"
      ],
      "metadata": {
        "id": "ZKilKSnLSBKh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What are the layers in a transformer block?\"\n",
        "# db2.similarity_search_with_score(query, k=3, fetch_k=1)"
      ],
      "metadata": {
        "id": "j1vBebCLeFfZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import Replicate\n",
        "\n",
        "llm = Replicate(\n",
        "    model=\"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
        "    model_kwargs={\"temperature\": 0.6, \"max_length\": 600, \"top_p\": 1},\n",
        ")"
      ],
      "metadata": {
        "id": "9xTZxIT8YMrj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
      ],
      "metadata": {
        "id": "VvJLBxsyYkQv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom_template = \"\"\"Using the given conversation and supporting documents, answer the question to the best of your knowledge. Also privide citings and refernces to the information you use. If you do not know the answer reply with 'I am sorry'.\n",
        "# Chat History: {chat_history}\n",
        "# Question: {question}\"\"\"\n",
        "\n",
        "# CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)"
      ],
      "metadata": {
        "id": "moJVzZNiLnf1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "#                                              retriever = db2.as_retriever(),\n",
        "#                                              chain_type=\"stuff\",\n",
        "#                                              condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
        "#                                              memory=memory)"
      ],
      "metadata": {
        "id": "VBmfm7KUZG7E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('---------------------------------------------------------------------------------')\n",
        "# print('Welcome to the NLQueryAgent. You are now ready to start interacting with your documents')\n",
        "# print('---------------------------------------------------------------------------------')\n",
        "\n",
        "# while True:\n",
        "#   query=input(f\"Prompt: \")\n",
        "#   if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
        "#     result = pdf_qa(\"question\": \"Please create a correct elaborate summary of our conversation history with key points\")\n",
        "#     print(f\"Summary: \" + result[\"answer\"])\n",
        "#     print('Exiting')\n",
        "#     sys.exit()\n",
        "#   if query == '':\n",
        "#     continue\n",
        "#   result = pdf_qa({\"question\": query})\n",
        "#   print(f\"Answer: \" + result[\"answer\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "56D5SbxqZAkT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qa(llm, chat_history, question):\n",
        "  context = db2.similarity_search(question, k = 2)\n",
        "  chunks = []\n",
        "  for cont in context:\n",
        "    chunks.append([\"information: \" + cont.page_content, \"source: \" + str(cont.metadata)])\n",
        "  prompt = \"\"\"\n",
        "    QUESTION: {question}\n",
        "    Context: {context}\n",
        "    Chat History: {chat_history}\n",
        "\n",
        "\n",
        "    Say Hi! The main goal is to answer the QUESTION using the chat history and context in less than 400 characters\n",
        "    Give page and source name from the contect of the information used\n",
        "    \"\"\"\n",
        "  final_question = prompt.format(question=question, context=context, chat_history=chat_history)\n",
        "  # print(final_question)\n",
        "  return llm(final_question)"
      ],
      "metadata": {
        "id": "481eb7iIrI_K"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------------------------------------------------------------------------------')\n",
        "print('Welcome to the NLQueryAgent. You are now ready to start interacting with your documents')\n",
        "print('---------------------------------------------------------------------------------')\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "  query=input(f\"Prompt: \")\n",
        "  if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
        "    q = \"Please create a correct elaborate summary of our conversation history with key points\"\n",
        "    ans = qa(llm, chat_history, q)\n",
        "    print(f\"Summary: \" + ans)\n",
        "    print('Exiting...')\n",
        "    break\n",
        "\n",
        "  if query == '':\n",
        "    continue\n",
        "\n",
        "  ans = qa(llm, chat_history, query)\n",
        "  chat_history.append((\"question: \" + query, \"response: \" + ans))\n",
        "  print(f\"Answer: \" + ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "NgHzEbRXW8It",
        "outputId": "23228c79-7a94-48c6-b4e7-36b92ad21835"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------\n",
            "Welcome to the NLQueryAgent. You are now ready to start interacting with your documents\n",
            "---------------------------------------------------------------------------------\n",
            "Prompt: What is penn state bank\n",
            "Answer:  Hello! The Penn Treebank is a classic dataset in natural language processing (NLP), originally annotated for syntactic parsing. It has been used in various research studies, including Emami and Jelinek (2004) and Mikolov and Zweig (2012). The dataset contains sentences with various grammatical structures, and is often used to evaluate the perplexity of language models. Perplexity refers to the difficulty of predicting a sentence or phrase, given its context. In NLP, perplexity is typically measured using a variety of metrics, such as language\n",
            "Prompt: What are types of transformers\n",
            "Answer:  Hi! There are two main types of transformers, namely:\n",
            "\n",
            "1. Recurrent Neural Networks (RNNs): These include Long Short-Term Memory (LSTMs) that allow the conditional distribution of a token to depend on the entire context.\n",
            "2. Transformers: A more recent architecture developed for machine translation in 2017, which again returned to having fixed context length but were much easier to train.\n",
            "\n",
            "Source: Page 5, Introduction _ CS324.pdf\n",
            "Prompt: exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReplicateError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9a0b5c67be72>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"q\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Please create a correct elaborate summary of our conversation history with key points\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Summary: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-a176d429d3ad>\u001b[0m in \u001b[0;36mqa\u001b[0;34m(llm, chat_history, question)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mfinal_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# print(final_question)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m             )\n\u001b[1;32m    877\u001b[0m         return (\n\u001b[0;32m--> 878\u001b[0;31m             self.generate(\n\u001b[0m\u001b[1;32m    879\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 )\n\u001b[1;32m    657\u001b[0m             ]\n\u001b[0;32m--> 658\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             output = (\n\u001b[0;32m--> 533\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    534\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             text = (\n\u001b[0;32m-> 1053\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/replicate.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mcompletion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/replicate.py\u001b[0m in \u001b[0;36m_create_prediction\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         }\n\u001b[0;32m--> 215\u001b[0;31m         return replicate_python.predictions.create(\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/prediction.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, version, input, webhook, webhook_completed, webhook_events_filter, stream, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         resp = self._client._request(\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;34m\"/v1/predictions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReplicateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"detail\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReplicateError\u001b[0m: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfPURopr9xmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}